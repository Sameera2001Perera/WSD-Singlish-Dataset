{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7845e983",
   "metadata": {},
   "source": [
    "Filter words in the transliteration dictionary based on the BERT vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "# Load the tokenizer and vocab\n",
    "model_directory = \"Ransaka/sinhala-bert-medium-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "bert_vocab = set(tokenizer.vocab.keys())\n",
    "\n",
    "# Input/output files\n",
    "# Define input and output file paths\n",
    "input_file = \"E:/4th Year/FYP/IMPLEMENTATION/data/dictionary.txt\"\n",
    "output_file = \"filtered_sinhala_words.txt\"\n",
    "\n",
    "# Helper: check if word's tokens are all in BERT vocab\n",
    "def is_word_in_vocab(word, vocab):\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    return all(token in vocab for token in tokens)\n",
    "\n",
    "# Dictionary parsing and filtering\n",
    "with open(input_file, 'r', encoding='utf-8') as fin, open(output_file, 'w', encoding='utf-8') as fout:\n",
    "    for line_number, line in enumerate(fin, 1):\n",
    "        if ':' not in line:\n",
    "            print(f\"Skipping line {line_number} (no colon): {line.strip()}\")\n",
    "            continue\n",
    "        try:\n",
    "            key, value = line.strip().split(':', 1)\n",
    "            key = key.strip()\n",
    "            sinhala_words = ast.literal_eval(value.strip())\n",
    "            if not isinstance(sinhala_words, list):\n",
    "                print(f\"Skipping line {line_number} (value is not a list): {line.strip()}\")\n",
    "                continue\n",
    "            # Filter words\n",
    "            filtered_words = [word for word in sinhala_words if is_word_in_vocab(word, bert_vocab)]\n",
    "            if filtered_words:\n",
    "                # Write in original format\n",
    "                fout.write(f\"{key}: {json.dumps(filtered_words, ensure_ascii=False)}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on line {line_number}: {line.strip()} -> {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9465b",
   "metadata": {},
   "source": [
    "- filter the output file using madhura and get \"Filtered_Sinhala_Dictionary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574e27a",
   "metadata": {},
   "source": [
    "- Since some madura api failed for several words, those were manually checked whether is there any spelling mistakes -> manually_checked_madura_filtered.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e4965",
   "metadata": {},
   "source": [
    "- Got the modified_lemma.txt file which contains Sinhala words and its lemma using verified_word_list_lemma_analysis.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c752fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reformat the lemma analysis data\n",
    "def reformat_lemma_analysis(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            # Split the line into lemma, frequency, and words\n",
    "            lemma_part, words_part = line.strip().split(':', 1)\n",
    "            lemma = lemma_part.strip()\n",
    "            words = words_part.split('[')[1].split(']')[0].replace(\"'\", \"\").split(', ')\n",
    "            \n",
    "            # Write each word with its corresponding lemma to the output file\n",
    "            for word in words:\n",
    "                outfile.write(f\"{word.strip()}: {lemma}\\n\")\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = 'E:/4th Year/FYP/IMPLEMENTATION/WSD/verified_word_list_lemma_analysis.txt'\n",
    "output_file = 'E:/4th Year/FYP/IMPLEMENTATION/WSD/modified_lemma.txt'\n",
    "\n",
    "# Reformat the lemma analysis data\n",
    "reformat_lemma_analysis(input_file, output_file)\n",
    "\n",
    "print(f\"Reformatted data has been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55840021",
   "metadata": {},
   "source": [
    "- Got the all_unique_words_in_verified_lemma file by using the words (not lemmas) from verified_word_list_lemma_analysis.txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e65ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract unique Sinhala words\n",
    "def extract_unique_words(input_file, output_file):\n",
    "    unique_words = set()  # Use a set to store unique words\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            # Split the line into lemma and words\n",
    "            lemma_part, words_part = line.strip().split(':', 1)\n",
    "            lemma = lemma_part.strip()\n",
    "            words = words_part.split('[')[1].split(']')[0].replace(\"'\", \"\").split(', ')\n",
    "            \n",
    "            # Add words to the set\n",
    "            # unique_words.add(lemma)\n",
    "            for word in words:\n",
    "                unique_words.add(word.strip())\n",
    "\n",
    "    # Write the unique words to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for word in sorted(unique_words):  # Sort the words alphabetically\n",
    "            outfile.write(f\"{word}\\n\")\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = 'E:/4th Year/FYP/IMPLEMENTATION/WSD/verified_word_list_lemma_analysis.txt'\n",
    "output_file = 'E:/4th Year/FYP/IMPLEMENTATION/WSD/all_unique_words_in_verified_lemma.txt'\n",
    "\n",
    "# Extract unique words\n",
    "extract_unique_words(input_file, output_file)\n",
    "\n",
    "print(f\"Unique Sinhala words have been written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d4bb5d",
   "metadata": {},
   "source": [
    "- filter the manually filtered dictionary Filtered_Sinhala_Dictionary and the all_unique_words_in_verified_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842e26b",
   "metadata": {},
   "source": [
    "- That output file was filtered again with the words in all_unique_words_in_verified_lemma.file -> dictionary_all_unique_verified_filtered.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7089d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the word list into a set for fast lookup\n",
    "with open('E:/4th Year/FYP/FYP-TEST/all_unique_words_in_verified_lemma.txt', 'r', encoding='utf-8') as file:\n",
    "    word_list = set(line.strip() for line in file)\n",
    "\n",
    "# Read the dictionary file and process it\n",
    "filtered_dictionary = {}\n",
    "with open('E:/4th Year/FYP/FYP-TEST/manually_checked_madura_filtered.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Split the line into the Romanized word and the list of Sinhala words\n",
    "        romanized_part, sinhala_part = line.strip().split(':')\n",
    "        romanized_word = romanized_part.strip()\n",
    "        sinhala_words = eval(sinhala_part.strip())  # Convert the string representation of the list to an actual list\n",
    "\n",
    "        # Filter the Sinhala words to keep only those present in the word list\n",
    "        filtered_sinhala_words = [word for word in sinhala_words if word in word_list]\n",
    "\n",
    "        # If there are any filtered words left, add them to the filtered dictionary\n",
    "        if len(filtered_sinhala_words)>1:\n",
    "            filtered_dictionary[romanized_word] = filtered_sinhala_words\n",
    "\n",
    "# Write the filtered dictionary to a new file\n",
    "with open('E:/4th Year/FYP/FYP-TEST/dictionary_all_unique_verified_filtered.txt', 'w', encoding='utf-8') as file:\n",
    "    for romanized_word, sinhala_words in filtered_dictionary.items():\n",
    "        file.write(f\"{romanized_word}: {sinhala_words}\\n\")\n",
    "\n",
    "print(\"Filtered dictionary has been saved to 'Filtered-Romanized-Sinhala-Sinhala-Dictionary.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee4501",
   "metadata": {},
   "source": [
    "- Created the lemmatized_final_dictionary.txt file by filtering the dictionary_all_unique_verified_filtered.txt with modified_lemma.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2d9d4",
   "metadata": {},
   "source": [
    "- create the lemmatized dictimonary considering the filtered(bert,dict,wordlist) and the modified lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a861053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import ast\n",
    "\n",
    "def load_dictionary(file_path):\n",
    "    \"\"\"Load the Romanized Sinhala to Sinhala dictionary from a text file.\"\"\"\n",
    "    dictionary = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            key, value = line.strip().split(': ', 1)\n",
    "            dictionary[key] = ast.literal_eval(value)  # Convert string list to actual list\n",
    "    return dictionary\n",
    "\n",
    "def load_lemmas(file_path):\n",
    "    \"\"\"Load the Sinhala words and their lemmas from a text file.\"\"\"\n",
    "    lemmas = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word, lemma = line.strip().split(': ')\n",
    "            lemmas[word] = lemma\n",
    "    return lemmas\n",
    "\n",
    "def group_by_lemma(dictionary, lemmas):\n",
    "    \"\"\"Group Sinhala words with the same lemma into tuples within the dictionary.\"\"\"\n",
    "    grouped_dict = {}\n",
    "    \n",
    "    for key, words in dictionary.items():\n",
    "        lemma_groups = defaultdict(list)\n",
    "        \n",
    "        for word in words:\n",
    "            # print(f\"Processing word: {word}\")\n",
    "            lemma = lemmas.get(word, word)  # Use word itself if lemma not found\n",
    "            # print(f\"Lemma: {lemma}\")\n",
    "            lemma_groups[lemma].append(word)\n",
    "            # print(f\"Lemma groups: {lemma_groups}\")\n",
    "        \n",
    "        # Convert lists to tuples if they contain more than one word\n",
    "        processed_words = []\n",
    "        single_words = []\n",
    "        for group in lemma_groups.values():\n",
    "            if len(group) > 1:\n",
    "                processed_words.append(tuple(group))\n",
    "            else:\n",
    "                single_words.extend(group)\n",
    "        \n",
    "        grouped_dict[key] = single_words + processed_words  # Ensure tuples are at the end\n",
    "    \n",
    "    return grouped_dict\n",
    "\n",
    "def save_dictionary(file_path, dictionary):\n",
    "    \"\"\"Save the processed dictionary to a text file.\"\"\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for key, value in dictionary.items():\n",
    "            file.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "# File paths\n",
    "romanized_sinhala_dict_path = 'E:/4th Year/FYP/FYP-TEST/dictionary_all_unique_verified_filtered.txt'\n",
    "lemma_file_path = 'E:/4th Year/FYP/FYP-TEST/modified_lemma.txt'\n",
    "output_file_path = 'E:/4th Year/FYP/FYP-TEST/lemmatized_final_dictionary.txt'\n",
    "\n",
    "# Load data\n",
    "dictionary = load_dictionary(romanized_sinhala_dict_path)\n",
    "lemmas = load_lemmas(lemma_file_path)\n",
    "\n",
    "# Process and group by lemma\n",
    "processed_dictionary = group_by_lemma(dictionary, lemmas)\n",
    "\n",
    "# Save output\n",
    "save_dictionary(output_file_path, processed_dictionary)\n",
    "\n",
    "print(\"Processed dictionary saved to\", output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5014a",
   "metadata": {},
   "source": [
    "- single element Sinhala lists were removed in lemmatized_final_dictionary.txt -> manually_checked_lemmatized_final_dictionary.txt (I dont remember why I used the \"manually\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1900a2a",
   "metadata": {},
   "source": [
    "code to filter the dictionary by removing words which less than 2 Sinhala words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the input and output file names\n",
    "input_file = \"E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/dic_verified_word_list2.txt\"\n",
    "output_file = 'E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/final_dictionary_2.txt'\n",
    "\n",
    "# Open the input file for reading and the output file for writing\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    # Iterate over each line in the input file\n",
    "    for line in infile:\n",
    "        # Split the line into the Romanized Sinhala word and the corresponding Sinhala words\n",
    "        romanized_word, sinhala_words = line.strip().split(':')\n",
    "        \n",
    "        # Evaluate the Sinhala words as a Python list\n",
    "        sinhala_words_list = eval(sinhala_words.strip())\n",
    "        \n",
    "        # Check if the Sinhala words list has at least 1 element\n",
    "        if len(sinhala_words_list) == 3:\n",
    "            # Write the line to the output file if it meets the condition\n",
    "            outfile.write(line)\n",
    "\n",
    "print(f\"Filtered dictionary has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12eb5d",
   "metadata": {},
   "source": [
    "- expand the variations of tuples in manually_checked_lemmatized_final_dictionary.txt -> manually_checked_lemmatized_final_dictionary2.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d1b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import itertools\n",
    "\n",
    "\n",
    "input_file = 'E:/4th Year/FYP/FYP-TEST/manually_checked_lemmatized_final_dictionary.txt'\n",
    "output_file = 'E:/4th Year/FYP/FYP-TEST/manually_checked_lemmatized_final_dictionary2.txt'\n",
    "\n",
    "\n",
    "# input_file = 'E:/4th Year/FYP/FYP-TEST/t1.txt'\n",
    "# output_file = 'E:/4th Year/FYP/FYP-TEST/t2.txt'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    \n",
    "    for line in infile:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip empty lines\n",
    "        \n",
    "        # Split the line into key and the list part\n",
    "        key_part, list_part = line.split(':', 1)\n",
    "        key = key_part.strip()\n",
    "        list_str = list_part.strip()\n",
    "        \n",
    "        try:\n",
    "            # Parse the list using ast.literal_eval\n",
    "            original_list = ast.literal_eval(list_str)\n",
    "        except:\n",
    "            print(f\"Skipping line due to parsing error: {line}\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare options for each element in the original list\n",
    "        options = []\n",
    "        for elem in original_list:\n",
    "            if isinstance(elem, tuple):\n",
    "                # Convert tuple to a list of its elements\n",
    "                options.append(list(elem))\n",
    "            else:\n",
    "                # Treat string as a single-element list\n",
    "                options.append([elem])\n",
    "        \n",
    "        # Generate all possible combinations using Cartesian product\n",
    "        for combination in itertools.product(*options):\n",
    "            # Convert the combination tuple to a list for representation\n",
    "            combination_list = list(combination)\n",
    "            # Write the line to the output file\n",
    "            outfile.write(f\"{key}: {repr(combination_list)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2ba0b",
   "metadata": {},
   "source": [
    "- created the dictionary_2 and dictionary_3 by deviding the manually_checked_lemmatized_final_dictionary2.txt into dictionaries which has 2 Sinhala words ad 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the input and output file names\n",
    "# input_file = \"E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/dic_verified_word_list2.txt\"\n",
    "# output_file = 'E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/final_dictionary_2.txt'\n",
    "\n",
    "# # Open the input file for reading and the output file for writing\n",
    "# with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "#     # Iterate over each line in the input file\n",
    "#     for line in infile:\n",
    "#         # Split the line into the Romanized Sinhala word and the corresponding Sinhala words\n",
    "#         romanized_word, sinhala_words = line.strip().split(':')\n",
    "        \n",
    "#         # Evaluate the Sinhala words as a Python list\n",
    "#         sinhala_words_list = eval(sinhala_words.strip())\n",
    "        \n",
    "#         # Check if the Sinhala words list has at least 1 element\n",
    "#         if len(sinhala_words_list) == 3:\n",
    "#             # Write the line to the output file if it meets the condition\n",
    "#             outfile.write(line)\n",
    "\n",
    "# print(f\"Filtered dictionary has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361d41f",
   "metadata": {},
   "source": [
    "- calculating and sorting frequencies for each Romanized Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast\n",
    "\n",
    "# # Read Sinhala word frequencies from word_list.si\n",
    "# frequency_dict = {}\n",
    "# with open('E:/4th Year/FYP/FYP-TEST/verified_word_list_200K.si', 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         parts = line.strip().split()\n",
    "#         if len(parts) < 2:\n",
    "#             continue\n",
    "#         word = parts[0]\n",
    "#         try:\n",
    "#             freq = int(parts[1])\n",
    "#             frequency_dict[word] = freq\n",
    "#         except (IndexError, ValueError):\n",
    "#             continue\n",
    "\n",
    "# # Read Romanized-Sinhala dictionary and process each entry\n",
    "# entries = []\n",
    "# with open('E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/dictionary_3.txt', 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         line = line.strip()\n",
    "#         if not line or ':' not in line:\n",
    "#             continue\n",
    "#         roman_part, sinhala_part = line.split(':', 1)\n",
    "#         roman = roman_part.strip()\n",
    "#         sinhala_part = sinhala_part.strip()\n",
    "#         try:\n",
    "#             sinhala_list = ast.literal_eval(sinhala_part)\n",
    "#             if not isinstance(sinhala_list, list):\n",
    "#                 continue\n",
    "#         except (SyntaxError, ValueError):\n",
    "#             continue\n",
    "#         # Calculate total frequency for this entry\n",
    "#         total = sum(frequency_dict.get(word, 0) for word in sinhala_list)\n",
    "#         entries.append((roman, sinhala_list, total))\n",
    "\n",
    "# # Sort entries by descending total, then Roman word, then Sinhala list\n",
    "# sorted_entries = sorted(entries, key=lambda x: (-x[2], x[0], str(x[1])))\n",
    "\n",
    "# # Write the sorted entries to a file\n",
    "# with open('E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/frequency_dict3.txt', 'w', encoding='utf-8') as f:\n",
    "#     for roman, sinhala_list, freq in sorted_entries:\n",
    "#         sinhala_str = str(sinhala_list)  # Converts list to string representation\n",
    "#         f.write(f\"{roman}: {sinhala_str}: {freq}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb9c4d",
   "metadata": {},
   "source": [
    "- remove word from dictionary which is not present n the verified word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9017846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Load the frequency list from word_list.si into a set\n",
    "def load_frequency_list(freq_file):\n",
    "    freq_words = set()\n",
    "    with open(freq_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.strip().split()[0]  # Extract the word (first column)\n",
    "            freq_words.add(word)\n",
    "    return freq_words\n",
    "\n",
    "# Load and filter the Romanized Sinhala dictionary\n",
    "def filter_dictionary(dict_file, freq_words, output_file):\n",
    "    filtered_dict = {}\n",
    "    \n",
    "    with open(dict_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue  # Skip empty lines\n",
    "            \n",
    "            key, value = line.split(':', 1)  # Split only on the first colon\n",
    "            key = key.strip()\n",
    "            value = ast.literal_eval(value.strip())  # Convert string list to actual list\n",
    "            \n",
    "            # Filter words based on frequency list\n",
    "            filtered_values = [word for word in value if word in freq_words]\n",
    "            \n",
    "            if filtered_values:\n",
    "                filtered_dict[key] = filtered_values\n",
    "    \n",
    "    # Save the filtered dictionary\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for key, values in filtered_dict.items():\n",
    "            f.write(f\"{key}: {values}\\n\")\n",
    "    \n",
    "    print(f\"Filtered dictionary saved to {output_file}\")\n",
    "\n",
    "# File paths\n",
    "romanized_dict_file = \"E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/dictionary_3.txt\"\n",
    "frequency_list_file = \"E:/4th Year/FYP/FYP-TEST/verified_word_lists/verified_word_list_3K.si\"\n",
    "output_file = \"E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/dic_verified_word_list3.txt\"\n",
    "\n",
    "\n",
    "# Process files\n",
    "freq_words = load_frequency_list(frequency_list_file)\n",
    "filter_dictionary(romanized_dict_file, freq_words, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output file names\n",
    "input_file = \"E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/dic_verified_word_list3.txt\"\n",
    "output_file = 'E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/final_dictionary_3.txt'\n",
    "\n",
    "# Open the input file for reading and the output file for writing\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    # Iterate over each line in the input file\n",
    "    for line in infile:\n",
    "        # Split the line into the Romanized Sinhala word and the corresponding Sinhala words\n",
    "        romanized_word, sinhala_words = line.strip().split(':')\n",
    "        \n",
    "        # Evaluate the Sinhala words as a Python list\n",
    "        sinhala_words_list = eval(sinhala_words.strip())\n",
    "        \n",
    "        # Check if the Sinhala words list has at least 1 element\n",
    "        if len(sinhala_words_list) > 1:\n",
    "            # Write the line to the output file if it meets the condition\n",
    "            outfile.write(line)\n",
    "\n",
    "print(f\"Filtered dictionary has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da80f7",
   "metadata": {},
   "source": [
    "- code to make dictionary with together the Singlish words with similar Sinhala word lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc31cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def normalize_word_list(word_list):\n",
    "    \"\"\"Normalize a word list by sorting it to handle order variations.\"\"\"\n",
    "    return tuple(sorted(word_list))\n",
    "\n",
    "def read_dictionary(file_path):\n",
    "    \"\"\"Read the dictionary file and return a grouped dictionary.\"\"\"\n",
    "    grouped_dict = defaultdict(list)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            romanized, sinhala_str = line.split(':')\n",
    "            sinhala_words = eval(sinhala_str.strip())  # Convert string to list\n",
    "            \n",
    "            normalized_key = normalize_word_list(sinhala_words)\n",
    "            grouped_dict[normalized_key].append(romanized.strip())\n",
    "    \n",
    "    return grouped_dict\n",
    "\n",
    "def write_grouped_dictionary(output_path, grouped_dict):\n",
    "    \"\"\"Write the grouped dictionary to a text file in the required format.\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for sinhala_words, romanized_list in grouped_dict.items():\n",
    "            f.write(f\"{', '.join(romanized_list)}: {list(sinhala_words)}\\n\")\n",
    "\n",
    "# Example usage\n",
    "file_path = \"E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/dictionary_3.txt\"\n",
    "output_path = \"E:/4th Year/FYP/FYP-TEST/lemmatized_dictionaries/grouped_final_dictionary_3.txt\"\n",
    "grouped_dictionary = read_dictionary(file_path)\n",
    "write_grouped_dictionary(output_path, grouped_dictionary)\n",
    "\n",
    "# Print result\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307790a5",
   "metadata": {},
   "source": [
    "- manually clean the file \"grouped_final_dictionary_3\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
